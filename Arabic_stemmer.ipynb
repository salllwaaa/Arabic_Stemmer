{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    # Tokenize the Arabic text\n",
        "    return re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "def remove_diacritics(text):\n",
        "    # Remove diacritics (vowel marks) from the Arabic text\n",
        "    diacritic_marks = ['َ', 'ُ', 'ِ', 'ْ', 'ّ','ٌ','ً','ٍ']\n",
        "    text=text.replace('أ', 'ا').replace('إ', 'ا').replace('آ', 'ا')\n",
        "    for mark in diacritic_marks:\n",
        "        text = text.replace(mark, '')\n",
        "    return text\n",
        "\n",
        "\n",
        "def normalize(word):\n",
        "    # Remove common Arabic prefixes and suffixes\n",
        "    prefixes = ['ال', 'و', 'ف', 'ب', 'ك', 'ل', 'ت']\n",
        "    suffixes = ['ة', 'ي', 'ه', 'نا', 'كم', 'هما', 'كن', 'ن', 'ا', 'ت']\n",
        "    prefixes.reverse()\n",
        "    suffixes.reverse()\n",
        "    for prefix in prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            word = word[len(prefix):]\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            word = word[:-len(suffix)]\n",
        "    word=word.replace('ى', 'ي')\n",
        "    return word\n",
        "\n",
        "def apply_prefix_rules(word):\n",
        "    # Apply the ISRI stemmer prefix rules to the word\n",
        "    # This involves removing additional prefixes based on the context of the word\n",
        "    # You can find the complete set of prefix rules in the ISRI stemmer paper\n",
        "    if word.startswith('و'):\n",
        "        if word[1:].startswith(('ل', 'س', 'ي', 'ن')):\n",
        "            word = word[1:]\n",
        "    if word.startswith(('الم', 'ال')):\n",
        "        word = word[3:]\n",
        "    if word.startswith('لل'):\n",
        "        word = word[2:]\n",
        "    return word\n",
        "\n",
        "def apply_suffix_rules(word):\n",
        "    # Apply the ISRI stemmer suffix rules to the word\n",
        "    # This involves removing additional suffixes based on the context of the word\n",
        "    # You can find the complete set of suffix rules in the ISRI stemmer paper\n",
        "    if word.endswith(('ات', 'ون', 'ين', 'تن', 'يه', 'ة','و')):\n",
        "        word = word[:-1]\n",
        "    if word.endswith('ان'):\n",
        "        if len(word) > 4:\n",
        "            word = word[:-2]\n",
        "        else:\n",
        "            word = word[:-1]\n",
        "    if word.endswith(('تما', 'تان', 'كما', 'هما', 'نا')):\n",
        "        word = word[:-2]\n",
        "    if word.endswith(('وا', 'يا', 'ا')):\n",
        "        word = word[:-1]\n",
        "    return word\n",
        "\n",
        "\n",
        "def stem(text):\n",
        "    # Tokenize the text into individual words\n",
        "    tokens = tokenize(text)\n",
        "    \n",
        "    # Stem each token\n",
        "    stemmed_tokens = []\n",
        "    for token in tokens:\n",
        "      if(token[-1]=='ى'):\n",
        "        token = remove_diacritics(token)\n",
        "        token = normalize(token)\n",
        "        token = apply_prefix_rules(token)\n",
        "      else: \n",
        "        token = remove_diacritics(token)\n",
        "        token = normalize(token)\n",
        "        token = apply_prefix_rules(token)\n",
        "        token = apply_suffix_rules(token)\n",
        "\n",
        "        # Add the stemmed token to the list\n",
        "      stemmed_tokens.append(token)\n",
        "     \n",
        "    # Return the stemmed text\n",
        "    return ' '.join(stemmed_tokens) "
      ],
      "metadata": {
        "id": "mCpCU6QP_5Uo"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "arabic_stop_words = set(stopwords.words('arabic'))\n",
        "\n",
        "text = 'الإيمان  ... وأثره على الصحة النفسية'\n",
        "words = text.split()\n",
        "\n",
        "\n",
        "filtered_words = [stem(word) for word in words if not word in arabic_stop_words]\n",
        "for word in filtered_words:\n",
        "  if word =='':\n",
        "    filtered_words.remove(word)\n",
        "\n",
        "print(filtered_words)\n",
        "\n",
        "#stem(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGBT13QtDVl-",
        "outputId": "3ce8ff52-5bb1-4fed-dbe5-d797c7661442"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ايم', 'اثر', 'صح', 'نفسي']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}