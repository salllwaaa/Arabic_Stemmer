{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samaasabri/Arabic_Stemmer/blob/main/Arabic_stemmer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def remove_non_arabic_chars(text):\n",
        "    # Remove non-Arabic characters from the text\n",
        "    output_string = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F]+', ' ', text)\n",
        "    return output_string\n",
        "    \n",
        "def tokenize(text):\n",
        "    # Tokenize the Arabic text\n",
        "    return re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "def remove_diacritics(text):\n",
        "    # Remove diacritics (vowel marks) from the Arabic text\n",
        "    diacritic_marks = ['َ', 'ُ', 'ِ', 'ْ', 'ّ','ٌ','ً','ٍ']\n",
        "    text=text.replace('أ', 'ا').replace('إ', 'ا').replace('آ', 'ا')\n",
        "    for mark in diacritic_marks:\n",
        "        text = text.replace(mark, '')\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def normalize(word):\n",
        "    # Remove common Arabic prefixes and suffixes\n",
        "    prefixes = ['ال', 'و', 'ف', 'ب', 'ك', 'ل', 'ت']\n",
        "    suffixes = ['ة', 'ي', 'ه', 'نا', 'كم', 'هما', 'كن', 'ن', 'ا', 'ت']\n",
        "    prefixes.reverse()\n",
        "    suffixes.reverse()\n",
        "    for prefix in prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            word = word[len(prefix):]\n",
        "            break\n",
        "\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            word = word[:-len(suffix)]\n",
        "            break\n",
        "    word=word.replace('ى', 'ي')\n",
        "    return word\n",
        "\n",
        "def apply_prefix_rules(word):\n",
        "    # Apply the ISRI stemmer prefix rules to the word\n",
        "    # This involves removing additional prefixes based on the context of the word\n",
        "    # You can find the complete set of prefix rules in the ISRI stemmer paper\n",
        "    if word.startswith('و'):\n",
        "        if word[1:].startswith(('ل', 'س', 'ي', 'ن')):\n",
        "            word = word[1:]\n",
        "    if word.startswith(('الم', 'ال')):\n",
        "        word = word[3:]\n",
        "    if word.startswith('لل'):\n",
        "        word = word[2:]\n",
        "    return word\n",
        "\n",
        "def apply_suffix_rules(word):\n",
        "    # Apply the ISRI stemmer suffix rules to the word\n",
        "    # This involves removing additional suffixes based on the context of the word\n",
        "    # You can find the complete set of suffix rules in the ISRI stemmer paper\n",
        "    if word.endswith(('ات', 'ون', 'ين', 'تن', 'يه', 'ة','و')):\n",
        "        word = word[:-1]\n",
        "    if word.endswith('ان'):\n",
        "        if len(word) > 4:\n",
        "            word = word[:-2]\n",
        "        else:\n",
        "            word = word[:-1]\n",
        "    if word.endswith(('تما', 'تان', 'كما', 'هما', 'نا')):\n",
        "        word = word[:-2]\n",
        "    if word.endswith(('وا', 'يا', 'ا')):\n",
        "        word = word[:-1]\n",
        "    return word\n",
        "\n",
        "\n",
        "def stem(text):\n",
        "    tokens = remove_non_arabic_chars(text)\n",
        "    # Tokenize the text into individual words\n",
        "    tokens = tokenize(tokens)\n",
        "    \n",
        "    # Stem each token\n",
        "    stemmed_tokens = []\n",
        "    for token in tokens:\n",
        "      if(len(token)>3):\n",
        "        if(token[-1]=='ى'):\n",
        "          token = remove_diacritics(token)\n",
        "          #token = remove_non_arabic_chars(token)\n",
        "          token = normalize(token)\n",
        "          token = apply_prefix_rules(token)\n",
        "        else: \n",
        "          token = remove_diacritics(token)\n",
        "          #token = remove_non_arabic_chars(token)\n",
        "          token = normalize(token)\n",
        "          token = apply_prefix_rules(token)\n",
        "          token = apply_suffix_rules(token)\n",
        "\n",
        "        # Add the stemmed token to the list\n",
        "      stemmed_tokens.append(token)\n",
        "     \n",
        "    # Return the stemmed text\n",
        "    return ' '.join(stemmed_tokens) "
      ],
      "metadata": {
        "id": "mCpCU6QP_5Uo"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "arabic_stop_words = set(stopwords.words('arabic'))\n",
        "\n",
        "\n",
        "arabic_stop_words.remove('علم')\n",
        "arabic_stop_words.remove('أبريل')\n",
        "arabic_stop_words.remove('أيلول')\n",
        "arabic_stop_words.remove('سبتمبر')\n",
        "arabic_stop_words.remove('ريال')\n",
        "arabic_stop_words.remove('أكتوبر')\n",
        "arabic_stop_words.remove('أغسطس')\n",
        "arabic_stop_words.remove('شمال')\n",
        "arabic_stop_words.remove('دولار')\n",
        "arabic_stop_words.remove('أمامكَ')\n",
        "arabic_stop_words.remove('يوليو')\n",
        "arabic_stop_words.remove('حبيب')\n",
        "arabic_stop_words.remove('فبراير')\n",
        "arabic_stop_words.remove('درهم')\n",
        "arabic_stop_words.remove('يونيو')\n",
        "\n",
        "text = '''اكان علم النفس دائما بالجامعات بتقاليده الدنيوية )غير\n",
        "الدينية( الملتزمة بالتنوير. وكان - على الدوام- من ضمن هذه\n",
        "التقاليد وجود شك واضح بكل أشكال التدين، كما يصف بيرنارد\n",
        ".غروم\n",
        "'''\n",
        "text2=\"الكلبان يلعبان في الحديقة eng\"\n",
        "text3=\"الكلبان يلعبان &*^%َ في الحديقة \"\n",
        "words = text3.split()\n",
        "\n",
        "\n",
        "filtered_words = [stem(word) for word in words if not word in arabic_stop_words]\n",
        "for word in filtered_words:\n",
        "  if word =='':\n",
        "    filtered_words.remove(word)\n",
        "output_text = ' '.join(filtered_words)\n",
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGBT13QtDVl-",
        "outputId": "dae3b71a-970b-4e17-8f42-039d813703a4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "كلب يلعب حديق\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}